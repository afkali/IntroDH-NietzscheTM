{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:17: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:17: DeprecationWarning: invalid escape sequence \\.\n",
      "C:\\Users\\husem\\AppData\\Local\\Temp\\ipykernel_11316\\2701334347.py:17: DeprecationWarning: invalid escape sequence \\.\n",
      "  papers['text'].map(lambda x: re.sub('[,\\.!?]', '', x))# Convert to lowercase\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\husem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deutsch', 'franzosischen', 'krieges', 'wahrend', 'donner', 'schlacht', 'worth', 'uber', 'europa', 'weggiengen', 'sass', 'grubler', 'rathselfreund', 'vaterschaft', 'buches', 'theil', 'ward', 'irgendwo', 'winkel', 'alpen', 'vergrubelt', 'verrathselt', 'folglich', 'bekummert', 'unbekummert', 'zugleich', 'schrieb', 'gedanken', 'uber', 'griechen']\n",
      "[(0, 13), (1, 8), (2, 2), (3, 2), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1)]\n",
      "[(0,\n",
      "  '0.005*\"uber\" + 0.004*\"menschen\" + 0.003*\"welt\" + 0.003*\"leben\" + '\n",
      "  '0.003*\"macht\" + 0.003*\"mensch\" + 0.003*\"selber\" + 0.002*\"gerade\" + '\n",
      "  '0.002*\"art\" + 0.002*\"schon\"'),\n",
      " (1,\n",
      "  '0.006*\"menschen\" + 0.005*\"uber\" + 0.004*\"leben\" + 0.003*\"selber\" + '\n",
      "  '0.003*\"welt\" + 0.003*\"musik\" + 0.003*\"gerade\" + 0.003*\"mensch\" + '\n",
      "  '0.003*\"sei\" + 0.003*\"art\"'),\n",
      " (2,\n",
      "  '0.005*\"menschen\" + 0.004*\"leben\" + 0.004*\"uber\" + 0.003*\"selber\" + '\n",
      "  '0.003*\"welt\" + 0.003*\"mensch\" + 0.003*\"musik\" + 0.003*\"gerade\" + '\n",
      "  '0.003*\"macht\" + 0.002*\"sei\"'),\n",
      " (3,\n",
      "  '0.005*\"menschen\" + 0.005*\"uber\" + 0.004*\"leben\" + 0.003*\"selber\" + '\n",
      "  '0.003*\"art\" + 0.003*\"mensch\" + 0.003*\"welt\" + 0.003*\"wurde\" + 0.003*\"sei\" + '\n",
      "  '0.002*\"gerade\"'),\n",
      " (4,\n",
      "  '0.005*\"menschen\" + 0.005*\"uber\" + 0.004*\"gerade\" + 0.003*\"leben\" + '\n",
      "  '0.003*\"art\" + 0.003*\"mensch\" + 0.003*\"welt\" + 0.003*\"wer\" + 0.002*\"sei\" + '\n",
      "  '0.002*\"schon\"'),\n",
      " (5,\n",
      "  '0.006*\"uber\" + 0.006*\"menschen\" + 0.003*\"selber\" + 0.003*\"leben\" + '\n",
      "  '0.003*\"macht\" + 0.003*\"welt\" + 0.003*\"gerade\" + 0.002*\"kunst\" + 0.002*\"art\" '\n",
      "  '+ 0.002*\"konnen\"'),\n",
      " (6,\n",
      "  '0.005*\"menschen\" + 0.004*\"uber\" + 0.003*\"leben\" + 0.003*\"mensch\" + '\n",
      "  '0.003*\"selber\" + 0.003*\"gerade\" + 0.003*\"macht\" + 0.003*\"wer\" + 0.003*\"sei\" '\n",
      "  '+ 0.002*\"art\"'),\n",
      " (7,\n",
      "  '0.005*\"menschen\" + 0.004*\"uber\" + 0.004*\"leben\" + 0.003*\"gerade\" + '\n",
      "  '0.003*\"macht\" + 0.002*\"mensch\" + 0.002*\"selber\" + 0.002*\"welt\" + '\n",
      "  '0.002*\"grossen\" + 0.002*\"schon\"'),\n",
      " (8,\n",
      "  '0.005*\"menschen\" + 0.005*\"uber\" + 0.003*\"macht\" + 0.003*\"leben\" + '\n",
      "  '0.003*\"wurde\" + 0.002*\"welt\" + 0.002*\"mensch\" + 0.002*\"selber\" + '\n",
      "  '0.002*\"glauben\" + 0.002*\"sei\"'),\n",
      " (9,\n",
      "  '0.006*\"menschen\" + 0.004*\"uber\" + 0.003*\"leben\" + 0.003*\"gerade\" + '\n",
      "  '0.002*\"schon\" + 0.002*\"welt\" + 0.002*\"mensch\" + 0.002*\"sei\" + 0.002*\"wurde\" '\n",
      "  '+ 0.002*\"macht\"')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\husem\\pycharmprojects\\nietzsche\\venv\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "papers = pd.read_csv('./data/output.tsv', sep='\\t', names=['titel', 'text'])\n",
    "\n",
    "# Remove punctuation\n",
    "papers['paper_text_processed'] = \\\n",
    "papers['text'].map(lambda x: re.sub('[,\\.!?]', '', x))# Convert to lowercase\n",
    "papers['paper_text_processed'] = \\\n",
    "papers['paper_text_processed'].map(lambda x: x.lower())# Print out the first rows of papers\n",
    "papers['paper_text_processed'].head()\n",
    "\n",
    "# Print out the first rows of papers\n",
    "papers['paper_text_processed'].head()\n",
    "\n",
    "# Join the different processed titles together.\n",
    "long_string = ','.join(list(papers['paper_text_processed'].values))# Create a WordCloud object\n",
    "\n",
    "stop_words = stopwords.words('german')\n",
    "stop_words.extend(['neue', 'ausgabe', 'versuch', 'selbstkritik', 'mehr', 'vonfriedrich', 'nietzsche', 'leipzig', 'verlag', 'fritzsch', 'fragwurdigen', 'buche', 'grunde', 'liegen', 'mag', 'frage', 'ersten', 'ranges', 'reizes', 'tief', 'personliche', 'frage', 'zeugniss', 'dafur', 'zeit', 'entstand', 'trotz', 'entstand', 'aufregende', 'fur', 'giebt', 'ja', 'immer', 'vielleicht', 'gar', 'denen' ])\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc))\n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "\n",
    "data = papers.paper_text_processed.values.tolist()\n",
    "data_words = list(sent_to_words(data))# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "print(data_words[:1][0][:30])\n",
    "\n",
    "import gensim.corpora as corpora# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)# Create Corpus\n",
    "texts = data_words# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]# View\n",
    "print(corpus[:1][0][:30])\n",
    "\n",
    "from pprint import pprint# number of topics\n",
    "num_topics = 10# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n",
    "\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_data_filepath = os.path.join('./data/ldavis_prepared_'+str(num_topics))\n",
    "\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "    pyLDAvis.save_html(LDAvis_prepared, './data/ldavis_prepared_'+ str(num_topics) +'.html')\n",
    "\n",
    "    LDAvis_prepared = pyLDAvis\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}